# -*- coding: utf-8 -*-
"""NLTK_Tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Unisvet/haf_ai/blob/main/NLTK_Tokenizer.ipynb

## Tokenization Example with NLTK

This script demonstrates a basic example of tokenization using the Natural Language Toolkit (NLTK) library in Python.

**Functionality:**

1. **Installation:** Installs the NLTK library using pip if it's not already installed.
2. **Import:** Imports necessary modules from NLTK: `word_tokenize` for word tokenization and `FreqDist` for frequency distribution (although not used in this specific example).
3. **Sample Text:** Defines a sample text string for tokenization.
4. **Data Download:** Downloads the 'punkt_tab' resource using `nltk.download('punkt_tab')`. This resource is essential for sentence tokenization, which is internally used by `word_tokenize`.
5. **Tokenization:** Tokenizes the sample text using `word_tokenize` and stores the resulting tokens in a list called `tokens`.
6. **Output:** Prints the generated tokens to the console.

**Usage:**

1. Ensure you have Python and pip installed.
2. Run the script. It will install NLTK if necessary, download the required data, and then perform tokenization on the sample text, printing the results.

**Dependencies:**

- NLTK (version 3.9.1)

**Note:**

The `punkt_tab` resource download is crucial for this script to function correctly. This resource provides the data required for sentence tokenization, which is a prerequisite for word tokenization using `word_tokenize`.
"""

# 1. Install NLTK
!pip install nltk

# 2. Import necessary modules
import nltk
from nltk.tokenize import word_tokenize
from nltk import FreqDist

# 3. Create a sample text
text = "This is a paragraph about tokenization. Tokenization is a common task in natural language processing. It involves splitting text into individual words or tokens. These tokens are then used for further analysis."

# Download the 'punkt_tab' resource
nltk.download('punkt_tab') # Download the necessary data for tokenization.
# 4. Tokenize the text
tokens = word_tokenize(text)

# 5. Print the tokens
print("Tokens:", tokens)

# 6. Count the number of tokens
num_tokens = len(tokens)
print("\nNumber of tokens:", num_tokens)

#7. Identify the frequency of each token
token_freq = FreqDist(tokens)
print("\nToken Frequency:", token_freq.most_common())